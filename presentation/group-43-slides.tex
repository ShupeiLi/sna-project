%! TeX program = lualatex
\documentclass[notes, 10pt, aspectratio=169]{beamer}
%\documentclass[10pt, aspectratio=169]{beamer}

\usepackage{booktabs}
\usepackage[style=verbose,backend=biber]{biblatex}
\addbibresource{../snacspaper.bib}

% notes
\usepackage{pgfpages}
\setbeamertemplate{note page}[plain]
\setbeameroption{show notes on second screen=right}
\graphicspath{{graphics/}}

\usetheme[style=fwn]{leidenuniv}
\useinnertheme{circles}
\useoutertheme[subsection=false]{miniframes}
\beamertemplatenavigationsymbolsempty

% uncomment next line to let framesubtitle have palette primary color
%\setbeamercolor{framesubtitle}{use={palette primary},fg=palette primary.bg}

% uncomment next line to remove navigation symbols from the pdf
%\setbeamertemplate{navigation symbols}{}

\title{node2vec: Scalable Feature Learning for Networks}
\subtitle{Authors: Aditya Grover and Jure Leskovec}
\author{Chenyu Shi and Shupei Li}
\institute[LIACS]{Leiden Institute of Advanced Computer Science}
\date{November 18, 2022}


\begin{document}

\begin{frame}[plain]
	\titlepage
\end{frame}
\note{This is the title page}

\begin{frame}
	\tableofcontents
\end{frame}
\note{This is the second page}

\section{Introduction}
\begin{frame}
    \frametitle{Introduction to Graph Embeddings}
    \begin{itemize}
        \item Represent graph-structured data.
        \item Applications: \\
            Social network analysis, recommender systems, molecular structure modelling, etc.
        \item Challenge: Limitations of traditional methods.
        \item Development of techniques specially for graph representations.\vspace{0.2cm}
            \begin{center}
                \includegraphics[width=10cm]{./graphics/intro.png}
            \end{center}
        \item node2vec is a feature learning framework.
    \end{itemize}
\end{frame}

\section{Related Work}
\begin{frame}
    \frametitle{Related Work}
    A taxonomy of graph embedding techniques \footcite{murphy2022}.
    \begin{center}
        \includegraphics[width=14cm]{./graphics/related-work.png}
    \end{center}
\end{frame}

\section{Methodology}
\begin{frame}
    \frametitle{Feature Learning Framework}
    \begin{itemize}
        \item Goal: Given a network $G=(V, E)$, find a projection $f:\ V \rightarrow R^d$.
        \item Generate a $d$-dimesion vector representation for each node.
        \item $f$ can be formulated as a matrix of size $\left| V \right| \cdot d$.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Feature Learning Framework}
    \begin{columns}
        \begin{column}{0.5\textwidth}
            Extending skip gram architecture to networks.\par ~\\
            Formulate feature learning in networks as a maximum likelihood optimization problem:
            \begin{align*}
                \max_{f}\ \sum_{u\in V}\log Pr\left( N_S\left( u \right)| f(u)  \right) 
            \end{align*}
            $N_S\left( a \right)$ is the network neighborhood set generated by neighborhood sampling strategy $S$ for node $a$.\par ~\\
            Important: $N_S\left( a \right)$ isn’t equavalent to direct local neighborhood.
        \end{column}
        \begin{column}{0.5\textwidth}
            For NLP: This is a [feature learning \alert{framework} social network].
            \begin{align*}
                \scriptstyle
                Pr \left( \left\{ \text{"feature"}, \text{"learning"}, \text{"social"}, \text{"network"} \right\} | \text{"framework"} \right) 
            \end{align*}
            For Graph:
            \begin{align*}
                N_S\left( a \right) &= \left\{ b, c, d, e \right\} \\
            Pr\left( \left\{ b, c, d, e \right\}| a \right) &= Pr\left( N_s\left( a \right) | a \right) 
            \end{align*}
            \pause
            Two problems to be solved:
            \begin{enumerate}
                \item How to define $N_S\left( a \right) $?
                \item How to compute $Pr\left( N_S\left( a \right) |a \right) $?
            \end{enumerate}
        \end{column}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Maximum Likehood Optimization}
    Formulate feature learning in networks as a maximum likelihood optimization problem:
    \begin{align*}
        \max_{f} \sum_{u\in V} \log Pr\left( N_S\left( u \right)|f(u)  \right) 
    \end{align*}
    Two standard assumptions:
    \begin{enumerate}
        \item Conditional independence:
            \begin{align*}
                Pr\left( N_S\left( u \right)| f\left( u \right)  \right) = \prod_{n_i\in N_S\left( u \right) }Pr\left( n_i|f(u) \right) 
            \end{align*}
        \item Symmetry in feature space:
            \begin{align*}
                Pr\left( n_i|f\left( u \right)  \right) = \frac{\exp\left( f\left( n_i \right) \cdot f\left( u \right)  \right) }{\sum_{v\in V}\exp\left( f\left( v \right)\cdot f\left( u \right)   \right)} 
            \end{align*}
    \end{enumerate}
\end{frame}

\begin{frame}
    \frametitle{Maximum Likehood Optimization}
    Finally, the optimization problem is converted into the form of:
    \begin{align*}
        \max_f \sum_{u\in V} \left[ -\log\left( \sum_{u\in V}\exp\left( f(u)\cdot f(v) \right)  \right) + \sum_{n_i\in N_S(u)} f(n_i)\cdot f(u) \right] 
    \end{align*}
    Use stochastic gradient decent to obtain projection $f$.
\end{frame}

\begin{frame}
    \frametitle{Network Neighborhood Sampling Strategy}
    Use classic search strategies:\\
    Breadth-first Sampling (BFS) and Depth-first Sampling (DFS).
    \begin{center}
        \includegraphics[width=6cm]{./graphics/BFS_DFS.png}
    \end{center}
    There are two kinds of similarities:
    \begin{enumerate}
        \item homophily (such as $u$ and $s_1$)
        \item structural equivalence (such as $u$ and $s_6$)
    \end{enumerate}
    \par \vspace{0.2cm}
    DFS tends to discover homophily, BFS tends to discover structural equivalence.\par \vspace{0.2cm}
    How to discover both kinds of similarities?
\end{frame}

\begin{frame}
    \frametitle{Network Neighborhood Sampling Strategy}
    Use basic random walk to discover both homophily and structural equalvalence similarities.\par \vspace{0.2cm}
    Basic random walk with length $l$ from source node $u$:
    \begin{align*}
        P\left( c_i=x|c_{i-1}=v \right) =
        \begin{cases}
            \frac{\pi_{vx}}{Z}\qquad & \text{if } \left( v, x \right) \in E\\
            0\qquad & \text{otherwise}
        \end{cases}
    \end{align*}
    \begin{itemize}
        \item[] $c_i$: the $i$-th node in the walk.
        \item[] $v$: current node.
        \item[] $\pi_{vx}$: unnormalized transition probability.
        \item[] $Z$: normalization constant.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Network Neighborhood Sampling Strategy}
    \begin{columns}
        \begin{column}{0.5\textwidth}<1->
            $\pi_{vx}$: Often set $\pi_{vx}=w_{vx}$ in weighted graphs.\\
            \phantom{$\pi_{vx}$: }In unweighted graph: $\pi_{vx}=1$.
            \begin{center}
                \includegraphics[width=5cm]{./graphics/Randomwalk.png}
            \end{center}
        \end{column}
        \begin{column}{0.5\textwidth}<2->
            Ramdom walk can combine features of DFS and BFS, and discovery both two kinds of similarities.\par ~\\
            Still not enough:\\
            It’s hard for us to guide and control the walking process.
        \end{column}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Network Neighborhood Sampling Strategy}
    \begin{columns}
        \begin{column}{0.5\textwidth}
         Use the second order bias random walk to get control of the walking process.
         \begin{align*}
             \pi_{vx} &= \alpha_{pq}\left( t, x \right) \cdot w_{vx}\\
             \alpha_{pq}\left( t, x \right) &=
             \begin{cases}
                 \frac{1}{p}\quad & \text{if } d_{tx} = 0\\
                 1\quad & \text{if } d_{tx} = 1\\
                 \frac{1}{q}\quad & \text{if } d_{tx} = 2
             \end{cases}
         \end{align*}
         \begin{itemize}
             \item[] $v$: current node.
             \item[] $t$: last node in the walk.
             \item[] $x$: next node to be chosen.
         \end{itemize}
        \end{column}
        \begin{column}{0.5\textwidth}
           \begin{center}
               \includegraphics[width=5cm]{./graphics/graphexample.png}
           \end{center} 
        \end{column}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Network Neighborhood Sampling Strategy}
    \begin{columns}
        \begin{column}{0.5\textwidth}
            \begin{itemize}
                \item[] $p$: return parameter.
                \item[] $q$: in-out parameter.
            \end{itemize}
            \begin{center}
                \includegraphics[width=5cm]{./graphics/biasrandomwalk.png}
            \end{center}
        \end{column}
        \begin{column}{0.5\textwidth}
            $p$ :
            \begin{itemize}
                \item High value: less likely to sample
an already visited node.
                \item Low value: likely to step back, then walk locally near the source node $u$.
            \end{itemize}
            $q$ :
            \begin{itemize}
                \item High value: biased towards nodes close to $t$, act more similarly to BFS.
                \item Low value: biased towards nodes distant to $t$, act more similarly to DFS.
            \end{itemize}
        \end{column}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Learning Edge Features}
    We have found a projection $f: V\rightarrow R^d$ with node2vec, which allocates each node vector embedding representation.\par \vspace{0.2cm}
    These embedding vectors can be used in node-related downstream tasks.\par \vspace{0.2cm}
    But how to learn edge features and deal with edge-related downstream tasks?\pause
    \begin{center}
        \begin{tabular}{lll}
            \toprule
            \textbf{Operator} & \textbf{Symbol} & \textbf{Definition}\\
            \midrule
            Average & $\boxplus$ & $\left[ f(u)\boxplus f(v) \right]_i = \frac{f_i(u) + f_i(v)}{2}$\\
            Hadamard & $\boxdot$ & $\left[ f(u)\boxdot f(v) \right]_i = f_i(u) \ast f_i(v) $\\
            Weighted-L1 & $\left\Vert \cdot \right\Vert_{\bar{1}}$ & $\left\Vert f(u) \cdot f(v) \right\Vert_{\bar{1}_i} = \left| f_i(u) - f_i(v) \right|$  \\ 
            Weighted-L2 & $\left\Vert \cdot \right\Vert_{\bar{2}}$ & $\left\Vert f(u) \cdot f(v) \right\Vert_{\bar{2}_i} = \left| f_i(u) - f_i(v) \right|^2$\\
            \bottomrule
        \end{tabular}
    \end{center}
    Given projection $f$ obtained by node2vec and two nodes $u$, $v$ along with edge $(u,v)$, apply \\ the binary operator on $f(u)$ and $f(v)$ to generate the representation $g(u,v)$, where $g: V\times V\rightarrow R^{d^{'}}$.
\end{frame}

\section{Experiments}
\begin{frame}
    \frametitle{Experiment 1: Multi-label Classification}
    \begin{itemize}
        \item Task description
            \begin{itemize}
                \item[$\circ$] Labels from a finite set $\mathcal{L}$
                \item[$\circ$] Training: A fraction of nodes and all their labels.
                \item[$\circ$] Predict the labels for the remaining nodes.
            \end{itemize}
        \item Data\par
            ~\\
            \begin{tabular}{llll}
                \toprule
                \textbf{Dataset} & \textbf{Nodes} & \textbf{Edges} & \textbf{Labels}\\
                \midrule
                BlogCatalog & 10,312 & 333,983 & 39\\
                Protein-Protein Interactions (PPI) & 3,890 & 76,584 & 50\\
                Wikipedia & 4,777 & 184,812 & 40\\
                \bottomrule
            \end{tabular}
            \par~\\
        \item Metrics: Macro-F1 score.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Experiment 1: Multi-label Classification}
    \begin{itemize}
        \item Results\par
            ~\\
            \begin{tabular}{llll}
                \toprule
                \textbf{Algorithm} & & \textbf{Dataset} &\\
                             & BlogCatalog & PPI & Wikipedia\\
                \midrule
                Spectral Clustering & 0.0405 & 0.0681 & 0.0395\\
                DeepWalk & 0.2110 & 0.1768 & 0.1274\\
                LINE & 0.0784 & 0.1447 & 0.1164\\
                node2vec & \textbf{0.2581} & \textbf{0.1791} & \textbf{0.1552}\\
                \midrule
                node2vec settings (p, q) & 0.25, 0.25 & 4, 1 & 4, 0.5\\
                Gain of node2vec [\%] & \textbf{22.3} & \textbf{1.3} & \textbf{21.8}\\
                \bottomrule
            \end{tabular}
            \par~\\
        \item node2vec outperforms the other benchmark algorithms.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Experiment 2: Link Prediction}
    \begin{itemize}
        \item Task description
            \begin{itemize}
                \item[$\circ$] A network with a fraction of edges removed.
                \item[$\circ$] Predict these missing edges.
            \end{itemize}
        \item Data\par
            ~\\
            \begin{tabular}{lll}
                \toprule
                \textbf{Dataset} & \textbf{Nodes} & \textbf{Edges}\\
                \midrule
                Facebook & 4,039 & 88,234 \\
                Protein-Protein Interactions (PPI) & 19,706 & 390,633\\
                arXiv ASTRO-PH & 18,722 & 198,110\\
                \bottomrule
            \end{tabular}
            \par~\\
        \item Metrics: Area Under Curve (AUC) score.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Experiment 2: Link Prediction}
    \begin{itemize}
        \item Results\par
            ~\\
            \begin{tabular}{llll}
                \toprule
                \textbf{Algorithm} & & \textbf{Dataset} & \\
                             & Facebook & PPI & arXiv \\
                \midrule
                Common Neighbors & 0.8100 & 0.7142 & 0.8153\\
                Jaccard's Coefficient & 0.8880 & 0.7018 & 0.8067\\
                Adamic-Adar & 0.8289 & 0.7126 & 0.8315\\
                Pref. Attachment & 0.7137 & 0.6670 & 0.6996\\
                Spectral Clustering & 0.6192 & 0.4920 & 0.5740\\
                DeepWalk & \textbf{0.9680} & 0.7441 & 0.9340\\
                LINE & 0.9490 & 0.7249 & 0.8902\\
                node2vec & \textbf{0.9680} & \textbf{0.7719} & \textbf{0.9366}\\
                \bottomrule
            \end{tabular}
            \par~\\
        \item The learned feature representations outperform heuristic scores. node2vec achieves the best AUC.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Summary of node2vec}
    \begin{itemize}
        \item An efficient graph embedding learning algorithm.
        \item Search strategy: Both flexible and controllable exploring network neighborhoods.
    \end{itemize}
\end{frame}

\section{Our work}
\begin{frame}
    \frametitle{Our Contributions}
    \begin{enumerate}
        \item During the training of node2vec, the intermediate state of node embeddings is a black box.\par
        Our solution: Visualize the node embeddings during the training of node2vec with t-SNE technique.
        \item Randomly initialized inputs in GNN affect the robustness of model performance and extend the model training time.\par
            Our solution: Propose a novel method that uses the pretrained embeddings from node2vec as the meta information for GNN.
        \item Effectiveness of algorithms.\par
            Our solution: Evaluate node2vec, GNN, and our proposed method on five real-world data sets with metrics that are different from the original paper.
    \end{enumerate}
\end{frame}

\begin{frame}
   \begin{columns}
       \begin{column}{0.33\textwidth}
           \centering
           \includegraphics[width=0.9\textwidth]{./graphics/epoch0.png}
           \includegraphics[width=0.9\textwidth]{./graphics/epoch60.png}
       \end{column}
       \begin{column}{0.33\textwidth}
           \centering
           \includegraphics[width=0.9\textwidth]{./graphics/epoch20.png}
           \includegraphics[width=0.9\textwidth]{./graphics/epoch80.png}
       \end{column}
       \begin{column}{0.33\textwidth}
           \centering
           \includegraphics[width=0.9\textwidth]{./graphics/epoch40.png}
           \includegraphics[width=0.9\textwidth]{./graphics/epoch100.png}
       \end{column}
   \end{columns} 
   \vspace{0.2cm}
   \begin{columns}
       \begin{column}{0.5\textwidth}
           AIFB dataset: Each node has a category label. There are 4 classes in total.
       \end{column}
       \begin{column}{0.5\textwidth}
           During the training of node2vec, nodes embedding are changed from chaos into order.
       \end{column}
   \end{columns}
\end{frame}


\begin{frame}
    \frametitle{Proposed Method}
    \begin{columns}
        \begin{column}{0.7\textwidth}
            node2vec + GNN
            \begin{itemize}
                \item Inspired by the concept of meta learning.
                \item An improved version of GNN.
                \item Choose the graph convolutional network (GCN) in our experiments.
                    \begin{itemize}
                        \item[$\circ$] GCN iteration formula:
                            \begin{align*}
                                h^{(k)} = \sigma (\hat{D}^{-\frac{1}{2}}\hat{A}\hat{D}^{-\frac{1}{2}}h^{(k-1)}W^{(k)})
                            \end{align*}
                            with $\hat{A} = A + I$, where $A$ is the adjacency matrix and $\hat{D}$ is the diagonal node degree matrix of $\hat{A}$.
            \end{itemize}
    \end{itemize}
        \end{column}
        \begin{column}{0.3\textwidth}
           \begin{center}
               \includegraphics[height=6.5cm]{./graphics/proposed-model.png}
           \end{center} 
        \end{column}
    \end{columns}
\end{frame}

\section{Future work}
\begin{frame}
    \frametitle{Future work} 
    \begin{enumerate}
        \item Apply node2vec, GCN, and our proposed model to node classification and link prediction.
        \item Try different strategies of hyperparameter tuning.
    \end{enumerate}
    \vspace{1.5cm}
    \begin{center}
        \LARGE Thanks!
    \end{center}
\end{frame}

\section{Appendix}
\begin{frame}
    \frametitle{Graph Neural Network}
    \begin{columns}
        \begin{column}{0.5\textwidth}
            Graph Neural Network is a deep learning framework for graph.\par ~\\
            General GNN iteration formula:
            \begin{align*}
                \displaystyle
                h_u^{(k)} = \sigma (W_{\text{self}}^{(k)} h_u^{(k-1)} + W_n^{(k)}\sum_{v\in N(u)} h_v^{(k-1)} + b^{(k)} ) 
            \end{align*}
            \begin{itemize}
                \item[] $h_u^{(k)}$: $k$-th layer output embedding of node $u$.
                \item[] $W^{(k)}$: weights of $k$-th layer (trainable).
                \item[] $b^{(k)}$: bias of $k$-th layer (trainable).
                \item[] $\sigma$: activation function.
            \end{itemize}
        \end{column}
        \begin{column}{0.5\textwidth}
            \includegraphics[width=7cm]{./graphics/gnn.png}
            Aggregate: To aggregate embeddings of $u$’s neighborhood.\par \vspace{0.2cm}
            Update: To update $u$’s embeddings using aggregation result and previous $u$’s embeddings.
        \end{column}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Graph Neural Network}
    \begin{align*}
        \displaystyle
        h_u^{(k)} = \sigma( W_{\text{self}}^{(k)} h_u^{(k-1)} + W_n^{(k)}\sum_{v\in N(u)} h_v^{(k-1)} + b^{(k)} ) 
    \end{align*}
    Final output embeddings can be used for calculating predictions. Then, we can use these predictions to compute the loss and optimize parameters with back propagation.\par \vspace{0.2cm}
    Problem: How to produce $h_u^0$ for each node?
    \begin{enumerate}
        \item Use one-hot vector for each node.
            \begin{itemize}
                \item[-] Drawback: The total number of nodes is large. Using one-hot vector for each node will cause the input tensor to be very sparse.
            \end{itemize} 
        \item Transfer pretrained embeddings from other similar tasks.
            \begin{itemize}
                \item[-] Drawback: There can't always be a similar task with pretrained embeddings for every network.
            \end{itemize}
        \item Use a trainable embedding layer to allocate randomly initialized embeddings for each node.
            \begin{itemize}
                \item[-] Drawback: Randomly initialized embeddings could have negative impact on training process.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}
    \frametitle{Combine node2vec and GNN}
    node2vec can produce embeddings for each node with graph information.\par ~\\
    GNN lacks a good general method to initialize its input nodes’ embeddings.\par ~\\
    We can use node2vec to produce initial input nodes’ embeddings for GNN to improve training process of GNN and obtain better performance.\par ~\\
    It’s a general method because there is no specific requirement of graphs when applying node2vec and GNN.
\end{frame}
\end{document}
