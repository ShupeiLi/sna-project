%! TeX program = lualatex
\documentclass[notes, 10pt, aspectratio=169]{beamer}
%\documentclass[10pt, aspectratio=169]{beamer}

\usepackage{booktabs}
\usepackage[style=verbose,backend=biber]{biblatex}
\addbibresource{../snacspaper.bib}

% notes
\usepackage{pgfpages}
\setbeamertemplate{note page}[plain]
\setbeameroption{show notes on second screen=right}
\graphicspath{{graphics/}}

\usetheme[style=fwn]{leidenuniv}
\useinnertheme{circles}
\useoutertheme[subsection=false]{miniframes}
\beamertemplatenavigationsymbolsempty

% uncomment next line to let framesubtitle have palette primary color
%\setbeamercolor{framesubtitle}{use={palette primary},fg=palette primary.bg}

% uncomment next line to remove navigation symbols from the pdf
%\setbeamertemplate{navigation symbols}{}

\title{node2vec: Scalable Feature Learning for Networks}
\subtitle{Authors: Aditya Grover and Jure Leskovec}
\author{Chenyu Shi and Shupei Li}
\institute[LIACS]{Leiden Institute of Advanced Computer Science}
\date{November 18, 2022}


\begin{document}

\begin{frame}[plain]
	\titlepage
\end{frame}
\note{Hello, everyone! We are group 43. The paper we are going to present today is node2vec: scalable feature learning for networks, written by Grover and Leskovec.}

\begin{frame}
	\tableofcontents
\end{frame}
\note{This is a brief content of our presentation. Firstly, we will introduce some background knowledge about graph embeddings, followed by a review of related work. Then, we will explain the node2vec algorithm in detail and present experimental results in the original paper. After that, we will introduce the idea of our course project. This presentation is concluded with the future work we plan to do in the project.}

\section{Introduction}
\begin{frame}
    \frametitle{Introduction to Graph Embeddings}
    \begin{itemize}
        \item Represent graph-structured data.
        \item Applications: \\
            Social network analysis, recommender systems, molecular structure modelling, etc.
        \item Challenge: Limitations of traditional methods.
        \item Development of techniques specially for graph representations.\vspace{0.2cm}
            \begin{center}
                \includegraphics[width=10cm]{./graphics/intro.png}
            \end{center}
        \item node2vec is a feature learning framework.
    \end{itemize}
\end{frame}
\note{As we've seen in this course, complex relationships can be modelled by graphs. Graph-structured data appears in many modern applications like molecular structure modelling, social network analysis, recommender system, etc. However, if we want to leverage the information contained in graphs, we need to find a way to represent graph-structured data efficiently. Traditional statistical and machine learning methods are designed for extracting features from structured data, such as PCA, UMAP, t-SNE. Although these methods have achieved satisfactory performance on grid-like data, they are hard to be generalized to graph-structured data, because they higly depend on properties of Eucildean space. This challenge led to the development of techniques specially for graph representation. The figure on the slide illustrates the general idea of graph embedding learning method. Firstly, we obtain embeddings by mapping graphs onto a low dimension space. And then, we can use these features in some downstream tasks like node classification and link prediction. From this perspective, node2vec algorithm that we address in presentation is a feature learning framework.}

\section{Related Work}
\begin{frame}
    \frametitle{Related Work}
    A taxonomy of graph embedding techniques \footcite{murphy2022}.
    \begin{center}
        \includegraphics[width=14cm]{./graphics/related-work.png}
    \end{center}
\end{frame}
\note{This figure illustrates the taxonomy of graph embedding techniques, according to a textbook written by Murphy. There are two types of graph embedding techniques. They are shallow embedding method and deep embedding method. Shallow embedding methods use shallow encoding functions to map the original graph structure onto a Euclidean space and obtain the embedding matrix. If data is labelled, we can apply supervised algorithms to extract embeddings, for example, label propagation. However, labels are not avaliable or only partly avaliable in most cases, where we need to use unspervised or semi-supervised learning. These methods can be divided into distance-based method and outer product method further. Some representative algorithms are given in the figure. It is worth noting that node2vec algorithm we emphasize in presentation belongs to outer product-based method using skip-gram architecture, which is inspired by skip-gram model in natral language processing field. Deep embedding method usually refers to algorithms that learn graph features via graph neural networks, or GNN. GNN is a special class of artificial neural networks designed for graph-structured data. In our project experiments, we mainly use a method called graph convolutional networks, which is a popular GNN architecture.}

\section{Methodology}
\begin{frame}
    \frametitle{Feature Learning Framework}
    \begin{itemize}
    	\item node2vec is a feature learning framework in nature.
        \item Goal: Given a network $G=(V, E)$, find a projection $f:\ V \rightarrow R^d$.
        \item Generate a $d$-dimesion vector representation for each node.
        \item $f$ can be formulated as a matrix of size $\left| V \right| \cdot d$.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Feature Learning Framework}
    \begin{columns}[t]
        \begin{column}{0.5\textwidth}
            Extending skip gram architecture to networks.\par ~\\
            Formulate feature learning in networks as a maximum likelihood optimization problem:
            \begin{align*}
                \max_{f}\ \sum_{u\in V}\log Pr\left( N_S\left( u \right)| f(u)  \right) 
            \end{align*}
            $N_S\left( a \right)$ is the network neighborhood set generated by neighborhood sampling strategy $S$ for node $a$.\par ~\\
            Important: $N_S\left( a \right)$ isn’t equavalent to direct local neighborhood.
        \end{column}
        \begin{column}{0.5\textwidth}
            For NLP: Given a literal data: \textit{This is a [feature learning \alert{framework} social network]}.
            \begin{align*}
                \scriptstyle
                Pr \left( \left\{ \text{"feature"}, \text{"learning"}, \text{"social"}, \text{"network"} \right\} | \text{"framework"} \right) 
            \end{align*}
            For Graph data: 
            \begin{align*}
                N_S\left( a \right) &= \left\{ b, c, d, e \right\} \\
            Pr\left( \left\{ b, c, d, e \right\}| a \right) &= Pr\left( N_s\left( a \right) | a \right) 
            \end{align*}
            \begin{center}
                \vspace{-0.2cm}
                \includegraphics<1>[width=3.5cm, height=3cm]{./graphics/NLPGA.png}
            \end{center}
        \only<2>{Two problems to be solved:}
            \begin{enumerate}
                \item<2> How to define $N_S\left( a \right) $?
                \item<2> How to compute $Pr\left( N_S\left( a \right) |a \right) $?
            \end{enumerate}
        \end{column}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Maximum Likehood Optimization}
    Formulate feature learning in networks as a maximum likelihood optimization problem:
    \begin{align*}
        \max_{f} \sum_{u\in V} \log Pr\left( N_S\left( u \right)|f(u)  \right) 
    \end{align*}
    Two standard assumptions:
    \begin{enumerate}
        \item Conditional independence:
            \begin{align*}
                Pr\left( N_S\left( u \right)| f\left( u \right)  \right) = \prod_{n_i\in N_S\left( u \right) }Pr\left( n_i|f(u) \right) 
            \end{align*}
        \item Symmetry in feature space:
            \begin{align*}
                Pr\left( n_i|f\left( u \right)  \right) = \frac{\exp\left( f\left( n_i \right) \cdot f\left( u \right)  \right) }{\sum_{v\in V}\exp\left( f\left( v \right)\cdot f\left( u \right)   \right)} 
            \end{align*}
    \end{enumerate}
\end{frame}

\begin{frame}
    \frametitle{Maximum Likehood Optimization}
    Finally, the optimization problem is converted into the form of:
    \begin{align*}
        \max_f \sum_{u\in V} \left[ -\log\left( \sum_{u\in V}\exp\left( f(u)\cdot f(v) \right)  \right) + \sum_{n_i\in N_S(u)} f(n_i)\cdot f(u) \right] 
    \end{align*}
    Use stochastic gradient decent to obtain projection $f$.
\end{frame}

\begin{frame}
    \frametitle{Network Neighborhood Sampling Strategy}
    Use classic search strategies:\\
    Breadth-first Sampling (BFS) and Depth-first Sampling (DFS).
    \begin{center}
        \includegraphics[width=6cm]{./graphics/BFS_DFS.png}
    \end{center}
    There are two kinds of similarities:
    \begin{enumerate}
        \item homophily (such as $u$ and $s_1$)
        \item structural equivalence (such as $u$ and $s_6$)
    \end{enumerate}
    \par \vspace{0.2cm}
    DFS tends to discover homophily, BFS tends to discover structural equivalence.\par \vspace{0.2cm}
    How to discover both kinds of similarities?
\end{frame}

\begin{frame}
    \frametitle{Network Neighborhood Sampling Strategy}
    Use basic random walk to discover both homophily and structural equalvalence similarities.\par \vspace{0.2cm}
    Basic random walk with length $l$ from source node $u$:
    \begin{align*}
        P\left( c_i=x|c_{i-1}=v \right) =
        \begin{cases}
            \frac{\pi_{vx}}{Z}\qquad & \text{if } \left( v, x \right) \in E\\
            0\qquad & \text{otherwise}
        \end{cases}
    \end{align*}
    \begin{itemize}
        \item[] $c_i$: the $i$-th node in the walk.
        \item[] $v$: current node.
        \item[] $\pi_{vx}$: unnormalized transition probability.
        \item[] $Z$: normalization constant.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Network Neighborhood Sampling Strategy}
    \begin{columns}
        \begin{column}{0.5\textwidth}<1->
            $\pi_{vx}$: Often set $\pi_{vx}=w_{vx}$ in weighted graphs.\\
            \phantom{$\pi_{vx}$: }In unweighted graph: $\pi_{vx}=1$.
            \begin{center}
                \includegraphics[width=5cm]{./graphics/Randomwalk.png}
            \end{center}
        \end{column}
        \begin{column}{0.5\textwidth}<2->
            Random walk can combine features of DFS and BFS, and discovery both two kinds of similarities.\par ~\\
            Still not enough:\\
            It’s hard for us to guide and control the walking process.
        \end{column}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Network Neighborhood Sampling Strategy}
    \begin{columns}
        \begin{column}{0.5\textwidth}
         Use the second order bias random walk to get control of the walking process.
         \begin{align*}
             \pi_{vx} &= \alpha_{pq}\left( t, x \right) \cdot w_{vx}\\
             \alpha_{pq}\left( t, x \right) &=
             \begin{cases}
                 \frac{1}{p}\quad & \text{if } d_{tx} = 0\\
                 1\quad & \text{if } d_{tx} = 1\\
                 \frac{1}{q}\quad & \text{if } d_{tx} = 2
             \end{cases}
         \end{align*}
         \begin{itemize}
             \item[] $v$: current node.
             \item[] $t$: last node in the walk.
             \item[] $x$: next node to be chosen.
         \end{itemize}
        \end{column}
        \begin{column}{0.5\textwidth}
           \begin{center}
               \includegraphics[width=5cm]{./graphics/graphexample.png}
           \end{center} 
        \end{column}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Network Neighborhood Sampling Strategy}
    \begin{columns}
        \begin{column}{0.5\textwidth}
            \begin{itemize}
                \item[] $p$: return parameter.
                \item[] $q$: in-out parameter.
            \end{itemize}
            \begin{center}
                \includegraphics[width=5cm]{./graphics/biasrandomwalk.png}
            \end{center}
        \end{column}
        \begin{column}{0.5\textwidth}
            $p$ :
            \begin{itemize}
                \item High value: Less likely to sample
an already visited node.
                \item Low value: Likely to step back, then walk locally near the source node $u$.
            \end{itemize}
            $q$ :
            \begin{itemize}
                \item High value: Biased towards nodes close to $t$, act more similarly to BFS.
                \item Low value: Biased towards nodes distant to $t$, act more similarly to DFS.
            \end{itemize}
        \end{column}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Learning Edge Features}
    We have found a projection $f: V\rightarrow R^d$ with node2vec, which allocates each node vector embedding representation.\par \vspace{0.2cm}
    These embedding vectors can be used in node-related downstream tasks.\par \vspace{0.2cm}
    But how to learn edge features and deal with edge-related downstream tasks?\pause
    \begin{center}
        \begin{tabular}{lll}
            \toprule
            \textbf{Operator} & \textbf{Symbol} & \textbf{Definition}\\
            \midrule
            Average & $\boxplus$ & $\left[ f(u)\boxplus f(v) \right]_i = \frac{f_i(u) + f_i(v)}{2}$\\
            Hadamard & $\boxdot$ & $\left[ f(u)\boxdot f(v) \right]_i = f_i(u) \ast f_i(v) $\\
            Weighted-L1 & $\left\Vert \cdot \right\Vert_{\bar{1}}$ & $\left\Vert f(u) \cdot f(v) \right\Vert_{\bar{1}_i} = \left| f_i(u) - f_i(v) \right|$  \\ 
            Weighted-L2 & $\left\Vert \cdot \right\Vert_{\bar{2}}$ & $\left\Vert f(u) \cdot f(v) \right\Vert_{\bar{2}_i} = \left| f_i(u) - f_i(v) \right|^2$\\
            \bottomrule
        \end{tabular}
    \end{center}
    Given projection $f$ obtained by node2vec and two nodes $u$, $v$ along with edge $(u,v)$, apply \\ the binary operator on $f(u)$ and $f(v)$ to generate the representation $g(u,v)$, where $g: V\times V\rightarrow R^{d^{'}}$.
\end{frame}

\section{Experiments}
\begin{frame}
    \frametitle{Experiment 1: Multi-label Classification}
    \begin{itemize}
        \item Task description
            \begin{itemize}
                \item[$\circ$] Labels from a finite set $\mathcal{L}$
                \item[$\circ$] Training: A fraction of nodes and all their labels.
                \item[$\circ$] Predict the labels for the remaining nodes.
            \end{itemize}
        \item Data\par
            ~\\
            \begin{tabular}{llll}
                \toprule
                \textbf{Dataset} & \textbf{Nodes} & \textbf{Edges} & \textbf{Labels}\\
                \midrule
                BlogCatalog & 10,312 & 333,983 & 39\\
                Protein-Protein Interactions (PPI) & 3,890 & 76,584 & 50\\
                Wikipedia & 4,777 & 184,812 & 40\\
                \bottomrule
            \end{tabular}
            \par~\\
        \item Metrics: Macro-F1 score.
    \end{itemize}
\end{frame}
\note{Authors of the original paper conducted two experiments to verify the effectiveness of the node2vec algorithm. The first experiment is multi-label classification. Given a finite label set and a fraction of nodes with labels, the task is predicting the labels for the remaining nodes. They use three data sets: BlogCatalog, PPI, and Wikipedia. The table shows some statistics of data sets. Macro-F1 score is selected as metrics.}

\begin{frame}
    \frametitle{Experiment 1: Multi-label Classification}
    \begin{itemize}
        \item Results\par
            ~\\
            \begin{tabular}{llll}
                \toprule
                \textbf{Algorithm} & & \textbf{Dataset} &\\
                             & BlogCatalog & PPI & Wikipedia\\
                \midrule
                Spectral Clustering & 0.0405 & 0.0681 & 0.0395\\
                DeepWalk & 0.2110 & 0.1768 & 0.1274\\
                LINE & 0.0784 & 0.1447 & 0.1164\\
                node2vec & \textbf{0.2581} & \textbf{0.1791} & \textbf{0.1552}\\
                \midrule
                node2vec settings (p, q) & 0.25, 0.25 & 4, 1 & 4, 0.5\\
                Gain of node2vec [\%] & \textbf{22.3} & \textbf{1.3} & \textbf{21.8}\\
                \bottomrule
            \end{tabular}
            \par~\\
        \item node2vec outperforms the other benchmark algorithms.
    \end{itemize}
\end{frame}
\note{Spectral clustering, DeepWalk, and LINE are selected as benchmarks. From the table, we can see that node2vec outperforms other algorithms on all datasets. Besides, its relative performance gain is significant.}

\begin{frame}
    \frametitle{Experiment 2: Link Prediction}
    \begin{itemize}
        \item Task description
            \begin{itemize}
                \item[$\circ$] A network with a fraction of edges removed.
                \item[$\circ$] Predict these missing edges.
            \end{itemize}
        \item Data\par
            ~\\
            \begin{tabular}{lll}
                \toprule
                \textbf{Dataset} & \textbf{Nodes} & \textbf{Edges}\\
                \midrule
                Facebook & 4,039 & 88,234 \\
                Protein-Protein Interactions (PPI) & 19,706 & 390,633\\
                arXiv ASTRO-PH & 18,722 & 198,110\\
                \bottomrule
            \end{tabular}
            \par~\\
        \item Metrics: Area Under Curve (AUC) score.
    \end{itemize}
\end{frame}
\note{The second task is link prediction. Given a network with a fraction of edges removed, the task is predicting these missing edges. They also use three datasets, Facebook, PPI, and arXiv, in this experiment. Statistics of data sets are shown in the table. AUC score is used as metrics.}

\begin{frame}
    \frametitle{Experiment 2: Link Prediction}
    \begin{itemize}
        \item Results\par
            ~\\
            \begin{tabular}{llll}
                \toprule
                \textbf{Algorithm} & & \textbf{Dataset} & \\
                             & Facebook & PPI & arXiv \\
                \midrule
                Common Neighbors & 0.8100 & 0.7142 & 0.8153\\
                Jaccard's Coefficient & 0.8880 & 0.7018 & 0.8067\\
                Adamic-Adar & 0.8289 & 0.7126 & 0.8315\\
                Pref. Attachment & 0.7137 & 0.6670 & 0.6996\\
                Spectral Clustering & 0.6192 & 0.4920 & 0.5740\\
                DeepWalk & \textbf{0.9680} & 0.7441 & 0.9340\\
                LINE & 0.9490 & 0.7249 & 0.8902\\
                node2vec & \textbf{0.9680} & \textbf{0.7719} & \textbf{0.9366}\\
                \bottomrule
            \end{tabular}
            \par~\\
        \item The learned feature representations outperform heuristic scores. node2vec achieves the best AUC.
    \end{itemize}
\end{frame}
\note{When the original paper was published, there were no feature learning algorithms that had been used for link prediction. Therefore, authors set four heuristic scores as benchmarks. The first four lines of table shows the performance of heuristic scores. The other three benchmarks are the same as in the experiment 1. Authors tested average, hadamard, weighted-L1, and weighted-L2 operations on these three benchmarks as well as node2vec. Due to the limited space, table on the slide only shows the result of hadamard operation. We can see that the learned feature representations outperform heuristic scores. And node2vec achieves the best AUC score on all datasets.}

\begin{frame}
    \frametitle{Summary of node2vec}
    \begin{itemize}
        \item An efficient graph embedding learning algorithm.
        \item Search strategy: Both flexible and controllable exploring network neighborhoods.
    \end{itemize}
\end{frame}
\note{A brief summary of node2vec. It is an algorithm designed for extracting embedidngs from graphs. Results of experiments support the effectiveness of the algorithm. Its neighborhood search strategy is flexible and controllable. We can easily change the behavior of search by adjusting hyperparameter p and q.}

\section{Our work}
\begin{frame}
    \frametitle{Our Contributions}
    \begin{enumerate}
        \item During the training of node2vec, the intermediate state of node embeddings is a black box.\par
        Our solution: Visualize the node embeddings during the training of node2vec with t-SNE technique.
        \item Randomly initialized inputs in GNN affect the robustness of model performance and extend the model training time.\par
            Our solution: Propose a novel method that combines node2vec and GNN.
        \item Effectiveness of algorithms.\par
            Our solution: Evaluate node2vec, GNN, and our proposed method on five real-world data sets with metrics that are different from the original paper.
    \end{enumerate}
\end{frame}
\note{Next, I will talk about our contributions in the project. Firstly, the intermediate state of node embeddings during node2vec training is a black box. We try to visualize the node embeddings with t-SNE technique to provide some insights into node2vec. Secondly, we find that researchers in graph embedding learning field have focused more on deep embedding methods these years. However, randomly initialized inputs in GNN affect the robustness of model performance and extend the training time. To address the problem, we proposed a novel method that combines node2vec and GNN. Thirdly, to further test the effectiveness of algorithms, we evaluate node2vec, GNN, and our proposed method on five real world datasets and select different metrics.}

\begin{frame}
   \begin{columns}
       \begin{column}{0.33\textwidth}
           \centering
           \includegraphics[width=0.9\textwidth]{./graphics/epoch0.png}
           \includegraphics[width=0.9\textwidth]{./graphics/epoch60.png}
       \end{column}
       \begin{column}{0.33\textwidth}
           \centering
           \includegraphics[width=0.9\textwidth]{./graphics/epoch20.png}
           \includegraphics[width=0.9\textwidth]{./graphics/epoch80.png}
       \end{column}
       \begin{column}{0.33\textwidth}
           \centering
           \includegraphics[width=0.9\textwidth]{./graphics/epoch40.png}
           \includegraphics[width=0.9\textwidth]{./graphics/epoch100.png}
       \end{column}
   \end{columns} 
   \vspace{0.2cm}
   \begin{columns}
       \begin{column}{0.5\textwidth}
           AIFB dataset: Each node has a category label. There are 4 classes in total.
       \end{column}
       \begin{column}{0.5\textwidth}
           During the training of node2vec, nodes embedding are changed from chaos into order.
       \end{column}
   \end{columns}
\end{frame}
\note{We apply node2vec algorithm on an open source dataset AIFB. Results of visualization are showed on the slide. Each node in AIFB belongs to a class. The total number of classes is four. And we set the number of epochs to 100. It is easy to see that node embeddings are changed from chaos into order during the training of node2vec.}

\begin{frame}
    \frametitle{Proposed Method}
    \begin{columns}
        \begin{column}{0.7\textwidth}
            node2vec + GNN
            \begin{itemize}
                \item Inspired by the concept of meta learning.
                \item An improved version of GNN.
                \item Choose the graph convolutional network (GCN) in our experiments.
                    \begin{itemize}
                        \item[$\circ$] GCN iteration formula:
                            \begin{align*}
                                h^{(k)} = \sigma (\hat{D}^{-\frac{1}{2}}\hat{A}\hat{D}^{-\frac{1}{2}}h^{(k-1)}W^{(k)})
                            \end{align*}
                            with $\hat{A} = A + I$, where $A$ is the adjacency matrix and $\hat{D}$ is the diagonal node degree matrix of $\hat{A}$.
            \end{itemize}
    \end{itemize}
        \end{column}
        \begin{column}{0.3\textwidth}
           \begin{center}
               \includegraphics[height=6.5cm]{./graphics/proposed-model.png}
           \end{center} 
        \end{column}
    \end{columns}
\end{frame}
\note{As mentioned before, random initialization in GNN may not be a good strategy. Inspired by the concept of meta learning, we use the pretrained embeddings from node2vec as the meta information for GNN. Figure on the right illustrates the architecture of our proposed model. We expect our method will accelerate the training stage of GNN and improve the model performance. We choose graph convolutional network or GCN in our experiments. The iteration formula of GCN is given on the slide. Preliminary results we have obtained so far support the effectiveness of our proposed method.}

\section{Future work}
\begin{frame}
    \frametitle{Future work} 
    \begin{enumerate}
        \item Apply node2vec, GCN, and our proposed model to node classification and link prediction.
        \item Try different strategies of hyperparameter tuning.
    \end{enumerate}
\end{frame}
\note{Our future work will mainly focus on two tasks. Task one, apply node2vec, GCN, and our proposed model to node classification and link prediction. Task two, try different strategies of hyperparameter tuning. Due to the time limit, we omit technical details of GNN. If you are interested in GNN, we can discuss the principle of GNN together. Thank you. This's all of our presentation. Any questions?}

\section{Appendix}
\begin{frame}
    \frametitle{Graph Neural Network}
    \begin{columns}
        \begin{column}{0.5\textwidth}
            Graph Neural Network is a deep learning framework for graph.\par ~\\
            General GNN iteration formula:
            \begin{align*}
                \displaystyle
                h_u^{(k)} = \sigma (W_{\text{self}}^{(k)} h_u^{(k-1)} + W_n^{(k)}\sum_{v\in N(u)} h_v^{(k-1)} + b^{(k)} ) 
            \end{align*}
            \begin{itemize}
                \item[] $h_u^{(k)}$: $k$-th layer output embedding of node $u$.
                \item[] $W^{(k)}$: weights of $k$-th layer (trainable).
                \item[] $b^{(k)}$: bias of $k$-th layer (trainable).
                \item[] $\sigma$: activation function.
            \end{itemize}
        \end{column}
        \begin{column}{0.5\textwidth}
            \includegraphics[width=7cm]{./graphics/gnn.png}
            Aggregate: To aggregate embeddings of $u$’s neighborhood.\par \vspace{0.2cm}
            Update: To update $u$’s embeddings using aggregation result and previous $u$’s embeddings.
        \end{column}
    \end{columns}
\end{frame}

\begin{frame}
    \frametitle{Graph Neural Network}
    \begin{align*}
        \displaystyle
        h_u^{(k)} = \sigma( W_{\text{self}}^{(k)} h_u^{(k-1)} + W_n^{(k)}\sum_{v\in N(u)} h_v^{(k-1)} + b^{(k)} ) 
    \end{align*}
    Final output embeddings can be used for calculating predictions. Then, we can use these predictions to compute the loss and optimize parameters with back propagation.\par \vspace{0.2cm}
    Problem: How to produce $h_u^0$ for each node?
    \begin{enumerate}
        \item Use one-hot vector for each node.
            \begin{itemize}
                \item[-] Drawback: The total number of nodes is large. Using one-hot vector for each node will cause the input tensor to be very sparse.
            \end{itemize} 
        \item Transfer pretrained embeddings from other similar tasks.
            \begin{itemize}
                \item[-] Drawback: There can't always be a similar task with pretrained embeddings for every network.
            \end{itemize}
        \item Use a trainable embedding layer to allocate randomly initialized embeddings for each node.
            \begin{itemize}
                \item[-] Drawback: Randomly initialized embeddings could have negative impact on training process.
            \end{itemize}
    \end{enumerate}
\end{frame}

\begin{frame}
    \frametitle{Combine node2vec and GNN}
    node2vec can produce embeddings for each node with graph information.\par ~\\
    GNN lacks a good general method to initialize its input nodes’ embeddings.\par ~\\
    We can use node2vec to produce initial input nodes’ embeddings for GNN to improve training process of GNN and obtain better performance.\par ~\\
    It’s a general method because there is no specific requirement of graphs when applying node2vec and GNN.
\end{frame}
\end{document}
